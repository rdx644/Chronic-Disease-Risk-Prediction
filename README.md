# ğŸ©º Chronic Disease Risk Prediction System  

 ğŸš€ Overview  
Chronic Disease Risk Prediction System is a cloud-based, AI-driven healthcare platform that predicts the likelihood of developing chronic diseases such as diabetes or heart disease based on user inputs like age, BMI, blood pressure, and lifestyle habits.  

This project leverages **AWS serverless architecture** for data ingestion, transformation, and machine learning â€” making the process scalable, secure, and fully automated.  

---

## ğŸ§  Key Objectives
- Collect and process user data through a simple web interface.  
- Apply ML models to identify potential chronic disease risks.  
- Generate personalized preventive recommendations.  
- Deliver an end-to-end automated pipeline using AWS services.  

---

 âš™ï¸ System Architecture  

The system is designed as a serverless data pipeline  on AWS:
User Input (S3 Website Form)
â†“
API Gateway â†’ Lambda (Data Validation)
â†“
DynamoDB (Raw Data Storage)
â†“
Lambda (Parquet Conversion using PyArrow)
â†“
S3 (Data Lake)
â†“
AWS Glue (ETL + Preprocessing with PySpark)
â†“
Amazon SageMaker (Model Training & Prediction)
â†“
Lambda (Prediction & Recommendations)
â†“
DynamoDB (Results)
â†“
API Gateway (GET Request)
â†“
Dashboard (Risk Level + Suggestions)

---

## ğŸ§© AWS Services Used

| Service | Purpose |
|----------|----------|
| **Amazon S3** | Hosts the front-end website and stores data files (raw + processed). |
| **Amazon API Gateway** | Handles HTTP requests between the front-end and back-end. |
| **AWS Lambda** | Executes data validation, conversion, and prediction logic. |
| **Amazon DynamoDB** | NoSQL database for storing user data and prediction results. |
| **AWS Glue** | Performs ETL (Extract, Transform, Load) operations using PySpark. |
| **Amazon SageMaker** | Trains and deploys the Random Forest Regressor model. |

---

## ğŸ§® Machine Learning Model

The ML component is built using **Random Forest Regressor** due to its robustness and interpretability.

### **Input Features**
- **Demographic Data:** `age`, `gender`, `height_cm`, `weight_kg`, `bmi`, `region`
- **Lifestyle Factors:** `smoking_status`, `alcohol_consumption`, `physical_activity_level`, `diet_quality`, `sleep_hours_per_day`, `stress_level`
- **Health Metrics:** `family_history`, `existing_conditions`, `blood_pressure_sys`, `blood_pressure_dia`, `cholesterol_level`, `glucose_level`

### **Output**
- **Predicted Risk Score (0â€“100)**  
- **Risk Level:** Low ğŸŸ¢ / Medium ğŸŸ¡ / High ğŸ”´  
- **Personalized Recommendation**

---

## ğŸ§° Tech Stack

| Category | Tools |
|-----------|-------|
| **Frontend** | HTML, CSS, JavaScript |
| **Backend** | AWS Lambda, Python (boto3, pyarrow) |
| **Database** | Amazon DynamoDB |
| **ETL** | AWS Glue + PySpark |
| **Machine Learning** | Amazon SageMaker (Random Forest Regressor) |
| **Hosting** | Amazon S3 (Static Website) |
| **Monitoring** | CloudWatch Logs + IAM Roles |

---

## ğŸ”„ Data Flow Summary

1. User submits data through a web form hosted on **Amazon S3**.  
2. **API Gateway** sends this data to **Lambda** for validation.  
3. The validated data is stored in **DynamoDB**.  
4. Another Lambda function converts the DynamoDB data into a **Parquet file** using **PyArrow**.  
5. The Parquet file triggers an **AWS Glue** job for cleaning and preprocessing.  
6. Processed data is sent to **Amazon SageMaker**, where the model predicts the disease risk.  
7. **Lambda** writes the prediction results back into **DynamoDB**.  
8. The front-end fetches results via **API Gateway (GET request)** and displays them on the dashboard.

---

 ğŸ§ª Model Evaluation
| Metric | Value |
|--------|--------|
| **RÂ² Score** | 0.88 |
| **RMSE** | 0.09 |
| **MAPE** | < 8% |
| **Model Type** | Random Forest Regressor |

The model achieved strong predictive performance with balanced accuracy and interpretability.

---

 ğŸ“Š Dashboard Output

The final dashboard displays:
- Userâ€™s **risk score**
- **Risk category** (Low, Moderate, High)
- Personalized **health recommendations**

---

 ğŸ” Security & Monitoring
- **IAM roles** with least-privilege access for each service.  
- **CloudWatch** monitors Glue, Lambda, and SageMaker logs.  
- **S3 bucket policies** enforce encryption and access control.  

---

 ğŸ”® Future Enhancements
- Integration of **IoT-based health sensors** (e.g., Fitbit, smartwatch).  
- Implementation of **automated model retraining pipelines** using AWS Step Functions.  
- Development of a **mobile application** for real-time monitoring.  
- Expansion to multi-disease prediction using advanced ensemble models.  

---

 ğŸ§‘â€ğŸ’» How to Run the Project

 **1ï¸âƒ£ Front-End Setup**
- Upload `index.html` and `form.html` to an S3 bucket.  
- Enable static website hosting under S3 bucket **Properties**.  
- Note the public URL.

 **2ï¸âƒ£ Backend Setup**
- Create **API Gateway** endpoints for `POST /submitData` and `GET /getPrediction`.  
- Connect them to respective **Lambda functions**.  
- Configure IAM roles for Lambda access to DynamoDB and S3.

 **3ï¸âƒ£ Database and ETL**
- Create **DynamoDB** table for raw data.  
- Create another table for prediction results.  
- Set up **AWS Glue** job with S3 input and output paths.

 **4ï¸âƒ£ ML Model Deployment**
- Use **SageMaker Notebook** to train and deploy the Random Forest Regressor model.  
- Store model artifact in S3 and deploy it as an endpoint.

**5ï¸âƒ£ Test**
- Visit your S3 website URL.  
- Enter test data and click submit.  
- Check DynamoDB â†’ Glue â†’ SageMaker pipeline for workflow completion.  
- View prediction results on dashboard.

---

## ğŸ“š References
This project leverages:
- [AWS Glue Documentation](https://docs.aws.amazon.com/glue/latest/dg/)
- [Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/)
- [AWS Lambda Developer Guide](https://docs.aws.amazon.com/lambda/latest/dg/)
- [Random Forest Algorithm - scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)

---

## ğŸ“„ License
This project is open-source and available under the [MIT License](LICENSE).

---

## ğŸ’¬ Acknowledgements
Special thanks to AWS Educate resources and SRM University for providing the academic and technical support required for this project.


